# Mobilenet_HLS
Work in progress of a hardware accelerator for MobileNet V3 Large. The board used for the development is the Zynq UltraScale+ ZCU104 Evaluation Board (xczu7ev-ffvc1156-2-e).

This is the latest version of the accelerator design. The design process started with the creation of a function performing the convolution operation. That initial function would take the image input as three separate streams for the red, green and blue channels and the nature of the streams (the ability to only read each element once) dictated the form of the rest of the function. In an effort to design a function as modular as possible, the streams were dropped, since each consequent of the original network layers has a different number of input channels, while the outputs may be calculated in parallel or in sequence, depending on whether the convolution is depthwise separable or not. In spite of that, in the interest of moving forward with the design, the architecture was not altered to better suit the matrix-style inputs and outputs of each convolutional layer.

Also part of the streaming of the inputs was the access to DRAM in an element-wise manner to store the needed input values in their corresponding buffers. This was supposed to occur concurrently with the actual convolution operation, thus minimizing the time wasted for data to be read from and written to the memory, however the current state of the design does not allow for that concurrency to occur. This is mainly a b-RAM related issue and it is partly due to the limited amount of elements that can be simultaneously written to the buffers from memory and read from the buffers to use in the convolution, as well as due to the sheer size of the network that would requre (at least for the current design architecture) more b-RAM than the board has to offer.

Any per layer optimizations are discarded for the time being. Initial attempts for a faster, larger, more parallelized implementation resulted in a non implementable architecture as the FF, DSP and LUT requirements would skyrocket as soon as deeper network layers with many more input channels were added to the design. Again, the completion of the network was deemed more critical, so the optimizations were left for the end, when the baseline hardware requirements for the whole network will be known. 

Another initial design choixe that has changed and is about to change again is the number precision. In order to validate the results of the initial layer and ensure the proper operation of the designed architecture floating point arithmetic was used, so that the results could be compared to their corresponding results from the python model. Of course this was about to change as soon as we had correct values and it was replaced by fixed point numbers. This however would still result in very large (in terms of bit width) data types if we would like to achieve a small enough variance between the python and the HLS results. The first design choice to counter balance that was the creation of a template class, which could be defined and instantiated in a custom manner for each convolutional layer, with template arguments specifying exactly the data types needed for each variable, register or buffer, according to the input types and the operations in which each of them would be part of. Although somewhat helpful, this choice added complexity both in terms of programming, since manual calculation was necessary for a lot of layers and human error is a potent factor delaying real progress, as well as in terms of area planning, as it would require custom hardware for every layer. After all that, the b-ram utilization remained too high to continue with that approach, so it was imparative that the data that would be stored in any memory would need to be maximum 8 bits long. At this point the results are still compared to those from the fixed point tensorflow model, so the quantization and dequantization of every filter element and every input element needs to occur before the operation in which it is used. This way allows for 8 bit data width storage, full length data (the length that at the previous - fixed point - stage was found to give adequately accurate results) to be used in the operations. The downside of this now is the addition of multiple exta MACCs just for the interpretation of the data, which increases the latency significantly. Finally, to mitigate this behaviour, we need to drop the fixed point operations alltogether and perform integer-only operations, as described by the paper in which tensorflow lite was based. In that case, the results will be compared to those from the tensorflow lite model of mobilenet, so that accuracy will be kept within the same levels and this work is currently in progress.

Another attempt to reduce the b-RAM usage was the organizing of multiple layers in blocks. The thought behind that choice is that if run sequentially, after its completion each block will release the b-RAM it needs, leaving room for the next block. Though it initially appeared to offer some improvements, eventually this thought seems to be flawed, as the blocks are implemented such that the b-RAM necessary for their operation is "hard wired", not allowing for resource sharing. Resource sharing, or rather lack there of, in and of itself is another problem of the current architecture. As it currently stands, the designed model requires a lot of hardware resources, only a small fraction of which are operating at any sigle time. Although the initial plan was that everything would be run concurrently in such a fashion that the completion of each layer would allow for new data to be processed by said layer, achieving throughput of 1 image per layer-time instead of 1 image per model-time. As already described, the current design does not allow for such operation and a balance between concurrency and resource requirements is in line and will be researched more extensively as soon as the model is made to "fit" in its etirety within the fpga fabric. To that end, function inlining is being explored.

Since the main concern is currently the minimizing of the b-RAM, there is one final consideration about th design that will allow for reusage of arrays after the initial values stored in them are no longer useful and that is to use vector like arrays (1 x data_len) instead of 3d or 4d tables. Though not imediately obvious at the first design steps, an array long enough to fit the largest feature map of each of the blocks mentioned earlier will be able to accomodate any other, smaller feature map, irrespectively of the way its dimensions are organized (i.e. 2x2x56 and 7x24x24 will both fit in an array of 1x5000 elements). An added benefit of such an approach is believed to be the reduction of hardware resources. The current design has several if statements (which are most likely implemented as multiplexers) determining end-of-line behavior, most of which can be eliminated if the buffers are vectors instead of tables. This approach however requires a few somewhat big modifications to the way data is processed within each layer object, so applying it is postponed until it is determined what else will be affected by it and how.

Apart from the imediate needs described up to this point, there are also a couple other known issues that will sooner or later need to be addressed. The first of them is that in the case of the layers performing convolution with 5x5 filters, the zero padding is not handled correctly. The reason for it not getting imediate attention is that a more efficient padding option is explored, which is proposed in [5]. The other issue is that some infinite loops might appear when the entire model is synthesized, but these are easily fixable by giving correct data types as arguments to the template at the object instantiation. However, since the direction is to now create uniform layers using 8 bit data as described above, this issue will be solved upon full implementation of said functionality.

Having said all of the above, the designed architecture achieves a clock frequency of 333 MHz, but because of its sequential still nature, the lack of optimizations and the amount operations per layer, it takes a couple of seconds to produce a result even from the first block. As far as improvements go, logarithmic representation of data could further reduce the b-RAM requirements, while at the same time the design complexity could be reduced by the consequent replacement of multiplications by additions.
