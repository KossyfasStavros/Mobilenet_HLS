# Mobilenet_HLS
Work in progress of a hardware accelerator for MobileNet V3 Large. The board used for the development is the Zynq UltraScale+ ZCU104 Evaluation Board (xczu7ev-ffvc1156-2-e).

This is the latest version of the accelerator design. The design process started with the creation of a function performing the convolution operation. That initial function would take the image input as three separate streams for the red, green and blue channels and the nature of the streams (the ability to only read each element once) dictated the form of the rest of the function. In an effort to design a function as modular as possible, the streams were dropped, since each consequent of the original network layers has a different number of input channels, while the outputs may be calculated in parallel or in sequence, depending on whether the convolution is depthwise separable or not. In spite of that, in the interest of moving forward with the design, the architecture was not altered to better suit the matrix-style inputs and outputs of each convolutional layer.

Also part of the streaming of the inputs was the access to DRAM in an element-wise manner to store the needed input values in their corresponding buffers. This was supposed to occur concurrently with the actual convolution operation, thus minimizing the time wasted for data to be read from and written to the memory, however the current state of the design does not allow for that concurrency to occur. This is mainly a b-RAM related issue and it is partly due to the limited amount of elements that can be simultaneously written to the buffers from memory and read from the buffers to use in the convolution, as well as due to the sheer size of the network that would requre (at least for the current design architecture) more b-RAM than the board has to offer.

Any per layer optimizations are discarded for the time being. Initial attempts for a faster, larger, more parallelized implementation resulted in a non implementable architecture as the FF, DSP and LUT requirements would skyrocket as soon as deeper network layers with many more input channels were added to the design. Again, the completion of the network was deemed more critical, so the optimizations were left for the end, when the baseline hardware requirements for the whole network will be known. 

Another initial design choixe that has changed and is about to change again is the number precision. In order to validate the results of the initial layer and ensure the proper operation of the designed architecture floating point arithmetic was used, so that the results could be compared to their corresponding results from the python model. Of course this was about to change as soon as we had correct values and it was replaced by fixed point numbers. This however would still result in very large (in terms of bit width) data types if we would like to achieve a small enough variance between the python and the HLS results. The first design choice to counter balance that was the creation of a template class, which could be defined and instantiated in a custom manner for each convolutional layer, with template arguments specifying exactly the data types needed for each variable, register or buffer, according to the input types and the operations in which each of them would be part of. Although somewhat helpful, this choice added complexity both in terms of programming, since manual calculation was necessary for a lot of layers and human error is a potent factor delaying real progress, as well as in terms of area planning, as it would require custom hardware for every layer. After all that, the b-ram utilization remained too high to continue with that approach, so it was imparative that the data that would be stored in any memory would need to be maximum 8 bits long. At this point the results are still compared to those from the fixed point tensorflow model, so the quantization and dequantization of every filter element and every input element needs to occur before the operation in which it is used. This way allows for 8 bit data width storage, full length data (the length that at the previous - fixed point - stage was found to give adequately accurate results) to be used in the operations. The downside of this now is the addition of multiple exta MACCs just for the interpretation of the data, which increases the latency significantly. Finally, to mitigate this behaviour, we need to drop the fixed point operations alltogether and perform integer-only operations, as described by the paper in which tensorflow lite was based. In that case, the results will be compared to those from the tensorflow lite model of mobilenet, so that accuracy will be kept within the same levels and this work is currently in progress.

Another attempt to reduce the b-RAM usage was the organizing of multiple layers in blocks. The thought behind that choice is that if run sequentially, after its completion each block will release the b-RAM it needs, leaving room for the next block. Though it initially appeared to offer some improvements, eventually this thought seems to be flawed, as the blocks are implemented such that the b-RAM necessary for their operation is "hard wired", not allowing for resource sharing. Resource sharing, or rather lack there of, in and of itself is another problem of the current architecture. As it currently stands, the designed model requires a lot of hardware resources (
